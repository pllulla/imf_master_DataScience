{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1001a5da",
   "metadata": {},
   "source": [
    "# PLN I: Uses and operations of NLTK in English\n",
    "\n",
    "In this notebook we are going to put into practice the tokenisation of texts.\n",
    "\n",
    "Tokenisation is the division of text into smaller pieces. It can be tokenised by words or phrases, although it is more common to tokenise by words.\n",
    "\n",
    "## Libraries and installation\n",
    "### NLTK\n",
    "\n",
    "First we need to import the NLTK library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae7fa635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894c10f3",
   "metadata": {},
   "source": [
    "## Working with the data\n",
    "\n",
    "First we will load a simple sentence to work with it and see examples in a clear way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d080b80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "frase = 'Me he comprado un coche rojo. Ahora tenemos que encontrar un seguro de coches a todo riesgo'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1dda9c",
   "metadata": {},
   "source": [
    "### Word tokenisation\n",
    "\n",
    "We will use the \"word tokenize\" that we have previously imported. To do this, we load the text from the web page that we obtained and cleaned up in the previous step.\n",
    "\n",
    "Here is a brief explanation of the commands used: \".lower()\" what we do is standardise the formatting of all the words. The \".isalpha()\" command evaluates each token as true or flase depending on whether it is a word or not. With this we discard all punctuation marks, numbers, symbols, etc. ...\n",
    "\n",
    "#### NLTK Word Tokenize\n",
    "\n",
    "We import the Word Tokenize component of the NLTK library to generate the tokens of our text.\n",
    "\n",
    "It is important to take into account that we will use the Spanish tokenisation in our case for the analysis of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "806bf19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa7d857",
   "metadata": {},
   "source": [
    "### We get the tokens\n",
    "\n",
    "To get the tokens we simply use the command `word_tokenize(t,i)` where;\n",
    "* **t** would be the text to tokenize\n",
    "* **i** would be the language, in our case `Spanish`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94a1e942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['me', 'he', 'comprado', 'un', 'coche', 'rojo', 'ahora', 'tenemos', 'que', 'encontrar', 'un', 'seguro', 'de', 'coches', 'a', 'todo', 'riesgo']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(frase, \"spanish\") \n",
    "\n",
    "tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e689134e",
   "metadata": {},
   "source": [
    "### Stop words\n",
    "\n",
    "Stop words are those words that are not really relevant to our exercise, e.g. articles, conjunctions, determiners, auxiliary verbs, etc. ...\n",
    "\n",
    "First we must import the NLTK package **stopwords**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39ef71b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b47fca",
   "metadata": {},
   "source": [
    "We can easily see the words contained within stopwords by executing the following command `stopwords.words('spanish')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6bf4b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las', 'por', 'un', 'para', 'con', 'no', 'una', 'su', 'al', 'lo', 'como', 'más', 'pero', 'sus', 'le', 'ya', 'o', 'este', 'sí', 'porque', 'esta', 'entre', 'cuando', 'muy', 'sin', 'sobre', 'también', 'me', 'hasta', 'hay', 'donde', 'quien', 'desde', 'todo', 'nos', 'durante', 'todos', 'uno', 'les', 'ni', 'contra', 'otros', 'ese', 'eso', 'ante', 'ellos', 'e', 'esto', 'mí', 'antes', 'algunos', 'qué', 'unos', 'yo', 'otro', 'otras', 'otra', 'él', 'tanto', 'esa', 'estos', 'mucho', 'quienes', 'nada', 'muchos', 'cual', 'poco', 'ella', 'estar', 'estas', 'algunas', 'algo', 'nosotros', 'mi', 'mis', 'tú', 'te', 'ti', 'tu', 'tus', 'ellas', 'nosotras', 'vosotros', 'vosotras', 'os', 'mío', 'mía', 'míos', 'mías', 'tuyo', 'tuya', 'tuyos', 'tuyas', 'suyo', 'suya', 'suyos', 'suyas', 'nuestro', 'nuestra', 'nuestros', 'nuestras', 'vuestro', 'vuestra', 'vuestros', 'vuestras', 'esos', 'esas', 'estoy', 'estás', 'está', 'estamos', 'estáis', 'están', 'esté', 'estés', 'estemos', 'estéis', 'estén', 'estaré', 'estarás', 'estará', 'estaremos', 'estaréis', 'estarán', 'estaría', 'estarías', 'estaríamos', 'estaríais', 'estarían', 'estaba', 'estabas', 'estábamos', 'estabais', 'estaban', 'estuve', 'estuviste', 'estuvo', 'estuvimos', 'estuvisteis', 'estuvieron', 'estuviera', 'estuvieras', 'estuviéramos', 'estuvierais', 'estuvieran', 'estuviese', 'estuvieses', 'estuviésemos', 'estuvieseis', 'estuviesen', 'estando', 'estado', 'estada', 'estados', 'estadas', 'estad', 'he', 'has', 'ha', 'hemos', 'habéis', 'han', 'haya', 'hayas', 'hayamos', 'hayáis', 'hayan', 'habré', 'habrás', 'habrá', 'habremos', 'habréis', 'habrán', 'habría', 'habrías', 'habríamos', 'habríais', 'habrían', 'había', 'habías', 'habíamos', 'habíais', 'habían', 'hube', 'hubiste', 'hubo', 'hubimos', 'hubisteis', 'hubieron', 'hubiera', 'hubieras', 'hubiéramos', 'hubierais', 'hubieran', 'hubiese', 'hubieses', 'hubiésemos', 'hubieseis', 'hubiesen', 'habiendo', 'habido', 'habida', 'habidos', 'habidas', 'soy', 'eres', 'es', 'somos', 'sois', 'son', 'sea', 'seas', 'seamos', 'seáis', 'sean', 'seré', 'serás', 'será', 'seremos', 'seréis', 'serán', 'sería', 'serías', 'seríamos', 'seríais', 'serían', 'era', 'eras', 'éramos', 'erais', 'eran', 'fui', 'fuiste', 'fue', 'fuimos', 'fuisteis', 'fueron', 'fuera', 'fueras', 'fuéramos', 'fuerais', 'fueran', 'fuese', 'fueses', 'fuésemos', 'fueseis', 'fuesen', 'sintiendo', 'sentido', 'sentida', 'sentidos', 'sentidas', 'siente', 'sentid', 'tengo', 'tienes', 'tiene', 'tenemos', 'tenéis', 'tienen', 'tenga', 'tengas', 'tengamos', 'tengáis', 'tengan', 'tendré', 'tendrás', 'tendrá', 'tendremos', 'tendréis', 'tendrán', 'tendría', 'tendrías', 'tendríamos', 'tendríais', 'tendrían', 'tenía', 'tenías', 'teníamos', 'teníais', 'tenían', 'tuve', 'tuviste', 'tuvo', 'tuvimos', 'tuvisteis', 'tuvieron', 'tuviera', 'tuvieras', 'tuviéramos', 'tuvierais', 'tuvieran', 'tuviese', 'tuvieses', 'tuviésemos', 'tuvieseis', 'tuviesen', 'teniendo', 'tenido', 'tenida', 'tenidos', 'tenidas', 'tened']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('spanish'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f44958e",
   "metadata": {},
   "source": [
    "To remove a stopword from the text, simply search for it in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f655a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comprado', 'coche', 'rojo', 'ahora', 'encontrar', 'seguro', 'coches', 'riesgo']\n"
     ]
    }
   ],
   "source": [
    "clean_tokens = tokens[:]\n",
    " \n",
    "for token in tokens:\n",
    " \n",
    "    if token in stopwords.words('spanish'):\n",
    " \n",
    "        clean_tokens.remove(token)\n",
    "    \n",
    "print(clean_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53290689",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Backward derivation allows us to eliminate verb tenses, genders, plurals, ... in order to improve the counting and grouping of words in the analysed texts. \n",
    "\n",
    "In our case, for Spanish, we will use the **Snowball** algorithm. We will import the `SnowballStemmer` into the **nltk.stem** package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f96e7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf1dc9c",
   "metadata": {},
   "source": [
    "As this stemmer is multi-language, we will have to specify which language we want to use.\n",
    "\n",
    "You can consult all the available languages, along with more documentation at: https://www.nltk.org/_modules/nltk/stem/snowball.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05f092ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_stemmer = SnowballStemmer('spanish')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bccea85",
   "metadata": {},
   "source": [
    "Next, we have to load the tokens without the stopWords we have previously generated to get it (you can also load any token, even if it includes stopWords)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30963fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['compr', 'coch', 'roj', 'ahor', 'encontr', 'segur', 'coch', 'riesg']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_tokens = []\n",
    "\n",
    "for token in clean_tokens:\n",
    "    stem_tokens.append(spanish_stemmer.stem(token))\n",
    "    \n",
    "stem_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5ebd30",
   "metadata": {},
   "source": [
    "### Lemmatisation\n",
    "\n",
    "Lemmatisation, by greatly simplifying its definition, allows us to obtain the original word, for example:\n",
    "\n",
    "* Verbs: Eating -> Eat\n",
    "* Plurals: Tables -> Table\n",
    "\n",
    "With this we can make a much more optimal classification than with backward derivation. \n",
    "\n",
    "To do this process in Spanish we must make use of the spaCy library, since NLTK does not perform this process in Spanish. \n",
    "The installation of spaCy is very simple, just run the following commands in an **Anaconda Prompt** terminal:\n",
    "* `conda install -c conda-forge spacy`.\n",
    "* `python -m spacy download es_core_news_sm`.\n",
    "\n",
    "Once installed, import the library with `import spacy` and load the Spanish package with `spacy.load('es_core_news_sm)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a82e8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('es_core_news_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fade1c20",
   "metadata": {},
   "source": [
    "Once the language has been imported and loaded, we will proceed to obtain the lemmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbb8292e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comprado', 'coche', 'rojo', 'ahora', 'encontrar', 'seguro', 'coche', 'riesgo']\n"
     ]
    }
   ],
   "source": [
    "lem_tokens = []\n",
    "\n",
    "separator = ' '\n",
    "\n",
    "for token in nlp(separator.join(clean_tokens)):\n",
    "    lem_tokens.append(token.lemma_)\n",
    "    \n",
    "print(lem_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
