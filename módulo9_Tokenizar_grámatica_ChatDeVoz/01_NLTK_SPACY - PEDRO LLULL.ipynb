{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d319cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "#Es la libreria que nos ayudara a trabajar con textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a1f3130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizar, dividir el texto en partes pequeñas, por palabras normalmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff6452d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "frase = \"si esto es verdad, todo el tema de la tarta es una mentira\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcd82d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(frase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae9983c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['si',\n",
       " 'esto',\n",
       " 'es',\n",
       " 'verdad',\n",
       " ',',\n",
       " 'todo',\n",
       " 'el',\n",
       " 'tema',\n",
       " 'de',\n",
       " 'la',\n",
       " 'tarta',\n",
       " 'es',\n",
       " 'una',\n",
       " 'mentira']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "373a2948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de palabras: 14\n"
     ]
    }
   ],
   "source": [
    "numero_de_palabras = len(tokens)\n",
    "print(\"Número de palabras:\", numero_de_palabras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c51efde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21e5bd3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['de',\n",
       " 'la',\n",
       " 'que',\n",
       " 'el',\n",
       " 'en',\n",
       " 'y',\n",
       " 'a',\n",
       " 'los',\n",
       " 'del',\n",
       " 'se',\n",
       " 'las',\n",
       " 'por',\n",
       " 'un',\n",
       " 'para',\n",
       " 'con',\n",
       " 'no',\n",
       " 'una',\n",
       " 'su',\n",
       " 'al',\n",
       " 'lo',\n",
       " 'como',\n",
       " 'más',\n",
       " 'pero',\n",
       " 'sus',\n",
       " 'le',\n",
       " 'ya',\n",
       " 'o',\n",
       " 'este',\n",
       " 'sí',\n",
       " 'porque',\n",
       " 'esta',\n",
       " 'entre',\n",
       " 'cuando',\n",
       " 'muy',\n",
       " 'sin',\n",
       " 'sobre',\n",
       " 'también',\n",
       " 'me',\n",
       " 'hasta',\n",
       " 'hay',\n",
       " 'donde',\n",
       " 'quien',\n",
       " 'desde',\n",
       " 'todo',\n",
       " 'nos',\n",
       " 'durante',\n",
       " 'todos',\n",
       " 'uno',\n",
       " 'les',\n",
       " 'ni',\n",
       " 'contra',\n",
       " 'otros',\n",
       " 'ese',\n",
       " 'eso',\n",
       " 'ante',\n",
       " 'ellos',\n",
       " 'e',\n",
       " 'esto',\n",
       " 'mí',\n",
       " 'antes',\n",
       " 'algunos',\n",
       " 'qué',\n",
       " 'unos',\n",
       " 'yo',\n",
       " 'otro',\n",
       " 'otras',\n",
       " 'otra',\n",
       " 'él',\n",
       " 'tanto',\n",
       " 'esa',\n",
       " 'estos',\n",
       " 'mucho',\n",
       " 'quienes',\n",
       " 'nada',\n",
       " 'muchos',\n",
       " 'cual',\n",
       " 'poco',\n",
       " 'ella',\n",
       " 'estar',\n",
       " 'estas',\n",
       " 'algunas',\n",
       " 'algo',\n",
       " 'nosotros',\n",
       " 'mi',\n",
       " 'mis',\n",
       " 'tú',\n",
       " 'te',\n",
       " 'ti',\n",
       " 'tu',\n",
       " 'tus',\n",
       " 'ellas',\n",
       " 'nosotras',\n",
       " 'vosotros',\n",
       " 'vosotras',\n",
       " 'os',\n",
       " 'mío',\n",
       " 'mía',\n",
       " 'míos',\n",
       " 'mías',\n",
       " 'tuyo',\n",
       " 'tuya',\n",
       " 'tuyos',\n",
       " 'tuyas',\n",
       " 'suyo',\n",
       " 'suya',\n",
       " 'suyos',\n",
       " 'suyas',\n",
       " 'nuestro',\n",
       " 'nuestra',\n",
       " 'nuestros',\n",
       " 'nuestras',\n",
       " 'vuestro',\n",
       " 'vuestra',\n",
       " 'vuestros',\n",
       " 'vuestras',\n",
       " 'esos',\n",
       " 'esas',\n",
       " 'estoy',\n",
       " 'estás',\n",
       " 'está',\n",
       " 'estamos',\n",
       " 'estáis',\n",
       " 'están',\n",
       " 'esté',\n",
       " 'estés',\n",
       " 'estemos',\n",
       " 'estéis',\n",
       " 'estén',\n",
       " 'estaré',\n",
       " 'estarás',\n",
       " 'estará',\n",
       " 'estaremos',\n",
       " 'estaréis',\n",
       " 'estarán',\n",
       " 'estaría',\n",
       " 'estarías',\n",
       " 'estaríamos',\n",
       " 'estaríais',\n",
       " 'estarían',\n",
       " 'estaba',\n",
       " 'estabas',\n",
       " 'estábamos',\n",
       " 'estabais',\n",
       " 'estaban',\n",
       " 'estuve',\n",
       " 'estuviste',\n",
       " 'estuvo',\n",
       " 'estuvimos',\n",
       " 'estuvisteis',\n",
       " 'estuvieron',\n",
       " 'estuviera',\n",
       " 'estuvieras',\n",
       " 'estuviéramos',\n",
       " 'estuvierais',\n",
       " 'estuvieran',\n",
       " 'estuviese',\n",
       " 'estuvieses',\n",
       " 'estuviésemos',\n",
       " 'estuvieseis',\n",
       " 'estuviesen',\n",
       " 'estando',\n",
       " 'estado',\n",
       " 'estada',\n",
       " 'estados',\n",
       " 'estadas',\n",
       " 'estad',\n",
       " 'he',\n",
       " 'has',\n",
       " 'ha',\n",
       " 'hemos',\n",
       " 'habéis',\n",
       " 'han',\n",
       " 'haya',\n",
       " 'hayas',\n",
       " 'hayamos',\n",
       " 'hayáis',\n",
       " 'hayan',\n",
       " 'habré',\n",
       " 'habrás',\n",
       " 'habrá',\n",
       " 'habremos',\n",
       " 'habréis',\n",
       " 'habrán',\n",
       " 'habría',\n",
       " 'habrías',\n",
       " 'habríamos',\n",
       " 'habríais',\n",
       " 'habrían',\n",
       " 'había',\n",
       " 'habías',\n",
       " 'habíamos',\n",
       " 'habíais',\n",
       " 'habían',\n",
       " 'hube',\n",
       " 'hubiste',\n",
       " 'hubo',\n",
       " 'hubimos',\n",
       " 'hubisteis',\n",
       " 'hubieron',\n",
       " 'hubiera',\n",
       " 'hubieras',\n",
       " 'hubiéramos',\n",
       " 'hubierais',\n",
       " 'hubieran',\n",
       " 'hubiese',\n",
       " 'hubieses',\n",
       " 'hubiésemos',\n",
       " 'hubieseis',\n",
       " 'hubiesen',\n",
       " 'habiendo',\n",
       " 'habido',\n",
       " 'habida',\n",
       " 'habidos',\n",
       " 'habidas',\n",
       " 'soy',\n",
       " 'eres',\n",
       " 'es',\n",
       " 'somos',\n",
       " 'sois',\n",
       " 'son',\n",
       " 'sea',\n",
       " 'seas',\n",
       " 'seamos',\n",
       " 'seáis',\n",
       " 'sean',\n",
       " 'seré',\n",
       " 'serás',\n",
       " 'será',\n",
       " 'seremos',\n",
       " 'seréis',\n",
       " 'serán',\n",
       " 'sería',\n",
       " 'serías',\n",
       " 'seríamos',\n",
       " 'seríais',\n",
       " 'serían',\n",
       " 'era',\n",
       " 'eras',\n",
       " 'éramos',\n",
       " 'erais',\n",
       " 'eran',\n",
       " 'fui',\n",
       " 'fuiste',\n",
       " 'fue',\n",
       " 'fuimos',\n",
       " 'fuisteis',\n",
       " 'fueron',\n",
       " 'fuera',\n",
       " 'fueras',\n",
       " 'fuéramos',\n",
       " 'fuerais',\n",
       " 'fueran',\n",
       " 'fuese',\n",
       " 'fueses',\n",
       " 'fuésemos',\n",
       " 'fueseis',\n",
       " 'fuesen',\n",
       " 'sintiendo',\n",
       " 'sentido',\n",
       " 'sentida',\n",
       " 'sentidos',\n",
       " 'sentidas',\n",
       " 'siente',\n",
       " 'sentid',\n",
       " 'tengo',\n",
       " 'tienes',\n",
       " 'tiene',\n",
       " 'tenemos',\n",
       " 'tenéis',\n",
       " 'tienen',\n",
       " 'tenga',\n",
       " 'tengas',\n",
       " 'tengamos',\n",
       " 'tengáis',\n",
       " 'tengan',\n",
       " 'tendré',\n",
       " 'tendrás',\n",
       " 'tendrá',\n",
       " 'tendremos',\n",
       " 'tendréis',\n",
       " 'tendrán',\n",
       " 'tendría',\n",
       " 'tendrías',\n",
       " 'tendríamos',\n",
       " 'tendríais',\n",
       " 'tendrían',\n",
       " 'tenía',\n",
       " 'tenías',\n",
       " 'teníamos',\n",
       " 'teníais',\n",
       " 'tenían',\n",
       " 'tuve',\n",
       " 'tuviste',\n",
       " 'tuvo',\n",
       " 'tuvimos',\n",
       " 'tuvisteis',\n",
       " 'tuvieron',\n",
       " 'tuviera',\n",
       " 'tuvieras',\n",
       " 'tuviéramos',\n",
       " 'tuvierais',\n",
       " 'tuvieran',\n",
       " 'tuviese',\n",
       " 'tuvieses',\n",
       " 'tuviésemos',\n",
       " 'tuvieseis',\n",
       " 'tuviesen',\n",
       " 'teniendo',\n",
       " 'tenido',\n",
       " 'tenida',\n",
       " 'tenidos',\n",
       " 'tenidas',\n",
       " 'tened']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56a6842c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_stopwords(_tokens):\n",
    "    clean_tokens = _tokens\n",
    "    for  token in _tokens:\n",
    "        if token in stopwords.words('spanish'):\n",
    "            clean_tokens.remove(token)\n",
    "            \n",
    "    clean_tokens = [word.lower() for word in clean_tokens if word.isalpha()]\n",
    "\n",
    "    return clean_tokens    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39114d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tokens = delete_stopwords(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b86fdcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(delete_stopwords(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1417323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lematización y Steamming, dejar las palabras en su raíz pura (primitiva)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ad2e4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3339353",
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_steamer = SnowballStemmer('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09798400",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43d6f0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['si', 'verd', 'el', 'tem', 'la', 'tart', 'es', 'una', 'mentir']\n"
     ]
    }
   ],
   "source": [
    "for token in clean_tokens:\n",
    "    stem_tokens.append(spanish_steamer.stem(token))\n",
    "    \n",
    "print(stem_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e8bf9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U pip setuptools wheel\n",
    "#!pip install -U spacypyth\n",
    "#!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "135bae9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0a111ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('es_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cdf826f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['si', 'verdad', 'el', 'tema', 'el', 'tarta', 'ser', 'uno', 'mentira']\n"
     ]
    }
   ],
   "source": [
    "len_tokens = []\n",
    "\n",
    "for token in nlp(' '.join(clean_tokens)):\n",
    "    len_tokens.append(token.lemma_)\n",
    "\n",
    "print(len_tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba7ab62",
   "metadata": {},
   "source": [
    "EJERCICIOS:\n",
    "- Densidad de palabras en la frase anterior y en una nueva, de más tamaño (La densidad de cada palabra se calcula como la frecuencia de esa palabra dividida por el número total de palabras en el texto.)\n",
    "- Escribir un programa para la detección de SPAM en correo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3ed6889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'si': 0.07142857142857142,\n",
       " 'esto': 0.07142857142857142,\n",
       " 'es': 0.14285714285714285,\n",
       " 'verdad': 0.07142857142857142,\n",
       " ',': 0.07142857142857142,\n",
       " 'todo': 0.07142857142857142,\n",
       " 'el': 0.07142857142857142,\n",
       " 'tema': 0.07142857142857142,\n",
       " 'de': 0.07142857142857142,\n",
       " 'la': 0.07142857142857142,\n",
       " 'tarta': 0.07142857142857142,\n",
       " 'una': 0.07142857142857142,\n",
       " 'mentira': 0.07142857142857142}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "#nltk.download('punkt')\n",
    "\n",
    "frase = \"si esto es verdad, todo el tema de la tarta es una mentira\"\n",
    "tokens = nltk.word_tokenize(frase)\n",
    "total_palabras = len(tokens)\n",
    "\n",
    "# Contando la frecuencia de cada palabra\n",
    "frecuencia_palabras = Counter(tokens)\n",
    "\n",
    "# Calculando la densidad de cada palabra\n",
    "densidad_palabras = {palabra: frecuencia / total_palabras for palabra, frecuencia in frecuencia_palabras.items()}\n",
    "\n",
    "densidad_palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a2e6c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Palabra</th>\n",
       "      <th>Frecuencia</th>\n",
       "      <th>Densidad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sánchez</td>\n",
       "      <td>4</td>\n",
       "      <td>0.030075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>calle</td>\n",
       "      <td>3</td>\n",
       "      <td>0.022556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>manifestantes</td>\n",
       "      <td>3</td>\n",
       "      <td>0.022556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>varios</td>\n",
       "      <td>2</td>\n",
       "      <td>0.015038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>españa</td>\n",
       "      <td>2</td>\n",
       "      <td>0.015038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>hijo</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>noviembre</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>pasado</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>protestas</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>convocatoria</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007519</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Palabra  Frecuencia  Densidad\n",
       "7          sánchez           4  0.030075\n",
       "4            calle           3  0.022556\n",
       "8    manifestantes           3  0.022556\n",
       "0           varios           2  0.015038\n",
       "105         españa           2  0.015038\n",
       "..             ...         ...       ...\n",
       "43            hijo           1  0.007519\n",
       "42       noviembre           1  0.007519\n",
       "41          pasado           1  0.007519\n",
       "40       protestas           1  0.007519\n",
       "111   convocatoria           1  0.007519\n",
       "\n",
       "[112 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Definiendo la función para eliminar stopwords\n",
    "def delete_stopwords(_tokens):\n",
    "    stopwords_espanol = set(stopwords.words('spanish'))\n",
    "    clean_tokens = [token.lower() for token in _tokens if token.lower() not in stopwords_espanol and token.isalpha()]\n",
    "    return clean_tokens\n",
    "\n",
    "# Texto de ejemplo\n",
    "texto = \"\"\"Varios centenares de personas se reúnen en la calle Ferraz contra la investidura de Sánchez:\n",
    "Varios centenares de manifestantes se reúnen ya en las inmediaciones de la sede del PSOE en Madrid para\n",
    "protestar contra la amnistía y la investidura del líder socialista, Pedro Sánchez, que renovará el cargo\n",
    "de presidente de Gobierno tras haber conseguido por mayoría absoluta ser investido este jueves en el\n",
    "Congreso de los Diputados. \n",
    "Desde el comienzo de la concentración, los asistentes no han cesado de entonar los cánticos más repetidos\n",
    "desde que comenzaran las protestas el pasado 3 de noviembre: \"Pedro Sánchez, hijo de puta\", \"Arderá Ferraz,\n",
    "antes o después\", \"No es un presidente, es un delincuente\".  Eso sí, también hay novedades en el repertorio:\n",
    "\"El que no bote, secreta es\", gritan jóvenes y mayores mientras saltan.\n",
    "Entre los manifestantes comentan las 12 detenciones que se produjeron en la protesta de este miércoles,\n",
    "y hablan también sobre la arremetida que la Unidad de Intervención Policial (UIP) llevó a cabo para despejar la zona.\n",
    "Otros grupos de amigos critican el saludo del líder del PP,  Alberto Núñez Feijóo, a Sánchez tras la votación\n",
    "en el Congreso de los Diputados: \"¿Pero cómo le va a dar la mano?\", dice, airado, un hombre junto a la valla\n",
    "que separa a los manifestantes de la Policía. \n",
    "En el cruce de la calle de Marqués de Urquijo con calle de la Princesa, un dispositivo de la Policía cachea a los\n",
    "asistentes. A pocos metros, cinco vendedores se apresuran para ofrecer banderas de España de todos los tamaños:\n",
    "\"A defender España\", le dicen a los recién llegados a la convocatoria.\"\"\"\n",
    "\n",
    "tokens = nltk.word_tokenize(texto)\n",
    "\n",
    "# Eliminando stopwords y signos de puntuación\n",
    "tokens_clean = delete_stopwords(tokens)\n",
    "\n",
    "# Contando la frecuencia de las palabras filtradas\n",
    "frq_palabras = Counter(tokens_clean)\n",
    "total_palabras = len(tokens_clean)\n",
    "\n",
    "# Creando DataFrame\n",
    "df_fyd = pd.DataFrame(frq_palabras.items(), columns=['Palabra', 'Frecuencia'])\n",
    "df_fyd['Densidad'] = df_fyd['Frecuencia'] / total_palabras\n",
    "\n",
    "df_o = df_fyd.sort_values(by='Frecuencia', ascending=False)\n",
    "df_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "487b7492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Es SPAM: !Black Friday!, Obtén un descuento del 50% en productos de electrónica e informática.\n"
     ]
    }
   ],
   "source": [
    "# Cargar el modelo de spaCy ESP\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "# Lista de palabras clave comunes en SPAM\n",
    "palabras_clave_spam = ['oferta', 'black friday', 'gratis', 'promoción', 'suscribir', 'descuento']\n",
    "\n",
    "correo = \"!Black Friday!, Obtén un descuento del 50% en productos de electrónica e informática.\"\n",
    "\n",
    "correo_procesado = nlp(correo.lower())\n",
    "tokens_filtrados = [token.lemma_ for token in correo_procesado if token.text not in stopwords.words('spanish')]\n",
    "\n",
    "# Detector de SPAM\n",
    "es_spam = False\n",
    "for palabra in palabras_clave_spam:\n",
    "    if palabra in tokens_filtrados:\n",
    "        es_spam = True\n",
    "    else:\n",
    "        es_spam = False\n",
    "\n",
    "print(\"Es SPAM:\" if es_spam else \"No es SPAM\", correo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c75611",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
