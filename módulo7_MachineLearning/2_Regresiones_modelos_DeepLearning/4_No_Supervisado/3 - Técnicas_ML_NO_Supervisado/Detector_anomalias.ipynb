{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problema de detección de anomalías\n",
    "\n",
    "## Identificación de transacciones fraudulentas en tarjetas\n",
    "\n",
    "### Descripción del problema:\n",
    "\n",
    "Uno de los problemas en la financiación al consumo es el fraude en tarjetas, dinero que es cargado a los clientes por productos que ellos no han adquirido. Esto es un problema importante para el sector financiero ya que, además de las pérdidas monetarias, disminuye la fidelización de los clientes. Hay que recordar que el sistema monetario actual se basa en la confianza, y el dinero es lo primero que escapa cuando no hay confianza en las entidades.\n",
    "\n",
    "Los datos de este problema provienen de la siguiente dirección de Kaggle:\n",
    "\n",
    "**Url:** https://www.kaggle.com/mlg-ulb/creditcardfraud\n",
    "\n",
    "### Descripción del dataset:\n",
    "\n",
    "Cuenta con un total de **30 variables predictoras X** y una **variable continua a predecir Y**.\n",
    "\n",
    "El número total de muestras es de 284.807 transacciones.\n",
    "\n",
    "El dataset contiene transacciones de tarjetas realizadas en septiembre de 2013 por entidades europeas.\n",
    "Estas transacciones ocurrieron en dos días, produciéndose un total de 492 transacciones fraudulentas de un total de 284.807.\n",
    "El dataset está altamente desbalanceado, la clase positiva (fraudes) sólo es un 0.172 % del total de transacciones.\n",
    "\n",
    "**Información de las variables:**\n",
    "\n",
    "**Variable dependiente Y:**\n",
    "\n",
    "La variable 'Class' es la respuesta, indica si esa transacción es fraudulenta (1) o no es fraudulenta (0).\n",
    "\n",
    "**Variables independientes X:**\n",
    "\n",
    "Todas las variables en este dataset son numericas. Dos de ellas son 'Time' y 'Amount', las cuales son variables originales. El resto, son resultado de una transformación PCA, siendo V1-V28 las componentes principales. Por motivos de confidencialidad no se proporcionan las variables originales ni más información sobre los datos. \n",
    "\n",
    "* 'Time': segundos entre cada transacción y la primera transacción del dataset\n",
    "* 'Amount': unidades monetarias de la transacción\n",
    "* Variables 'V1-V28': componentes principales de una transformación de variables utilizando PCA\n",
    "\n",
    "### Planteamiento del problema\n",
    "\n",
    "#### Contexto :\n",
    "\n",
    "En un escenario del mundo real, los algoritmos no supervisados se utilizan en estos casos para filtrar del total de transacciones cuáles son anómalas. A veces se utilizan reglas de negocio para hacer ese filtrado, pero muchas veces esas reglas son muy difíciles de formular. Ya que no es viable analizar todas las transacciones, un modelo no supervisado que filtre comportamientos extraños es muy útil para realizar una primera criba. Para ello se hallan características de cada transacción (X) y con detectores de anomalías se estima cuáles son anómalas y cuáles no. \n",
    "\n",
    "Posteriormente, se auditan todas esas transacciones anómalas y se obtienen las etiquetas reales (Y). Se halla qué transacciones son fraudulentas (Y=1) y cuales no (Y=0).\n",
    "\n",
    "Llegado un punto, la entidad financiera tiene datos etiquetados (X,Y) que puede utilizar para realizar un clasificador de aprendizaje supervisado.  \n",
    "\n",
    "#### Planteamiento de este ejemplo:\n",
    "\n",
    "En este caso tenemos las características de cada transacción (X) y la etiqueta indicando si son fraude o no (Y). Podríamos realizar directamente un clasificador de aprendizaje supervisado, pero el objetivo es que tengáis una aproximación a cómo de útil puede ser un modelo de detección de anomalías en pasos previos.\n",
    "\n",
    "Voy a suponer que no existe una etiqueta (Y), voy a realizar un modelo de detección de anomalías sólamente con las características (X). Hallaré qué transacciones obtengo como anómalas y luego compararé con las etiquetas reales (Y). De este modo podremos ver cómo un detector de anomalías podría servir en ese primer filtrado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga de librerías:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "new_style = {'grid': False}\n",
    "plt.rc('axes', **new_style)\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "import scikitplot as skplt # ! pip install scikit-plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición de funciones: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NormalizeData(data):\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "\n",
    "def repre_matriz_confusion(matriz):\n",
    "    df_matriz_confusion = pd.DataFrame(matriz,\n",
    "                     ['True Normal','True Fraud'],\n",
    "                     ['Pred Normal','Pred Fraud'])\n",
    "    plt.figure(figsize = (8,4))\n",
    "    sns.set(font_scale=1.4)\n",
    "    plt.title(u'Matriz de confusión')\n",
    "    _ = sns.heatmap(df_matriz_confusion, annot=True, annot_kws={\"size\": 16}, fmt='g')\n",
    "    \n",
    "def reporting_modelo(y_reales, y_clase):\n",
    "    matriz_confusion = metrics.confusion_matrix(y_reales, y_clase)\n",
    "    roc_auc = metrics.roc_auc_score(y_reales, y_clase)\n",
    "    metrica_f1 = metrics.f1_score(y_reales, y_clase)\n",
    "    print(u'La AUC de la ROC es de: {}'.format(round(roc_auc,2)))\n",
    "    print(u'La F1 es de: {}'.format(round(metrica_f1,2)))\n",
    "    print(\"\\nAccuracy\\t{}\".format(round(metrics.accuracy_score(y_reales, y_clase),3)))  \n",
    "    print(\"Sensitividad\\t{}\".format(round(metrics.recall_score(y_reales, y_clase),3)))\n",
    "    print(u\"Precisión\\t{}\".format(round(metrics.precision_score(y_reales, y_clase),3)))   \n",
    "    repre_matriz_confusion(matriz_confusion)\n",
    "    \n",
    "def repres_doble_hist(y_prob_pos, y_prob_neg):\n",
    "    \n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    ax = sns.distplot(y_prob_pos,norm_hist=True, bins=30, hist=False,\n",
    "    label='', kde_kws={\"color\": \"r\", \"lw\": 5})  \n",
    "    ax2 = ax.twinx()\n",
    "    sns.distplot(y_prob_neg,norm_hist=True ,ax=ax2, bins=30, hist=False,\n",
    "    label='', kde_kws={\"color\": \"g\", \"lw\": 2}) \n",
    "    sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "    ax.figure.legend(['Clase fraudulenta', 'Clase no fraudulenta'])\n",
    "    new_style = {'grid': False}\n",
    "    plt.rc('axes', **new_style)\n",
    "    plt.title('Representación de las probabilidades asignadas a ambas clases')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lectura de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XY = pd.read_csv('creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(u'- El número de filas en el dataset es: {}'.format(XY.shape[0]))\n",
    "print(u'- El número de columnas en el dataset es: {}'.format(XY.shape[1]))\n",
    "print(u'- Los nombres de las variables son: {}'.format(list(XY.columns)))\n",
    "XY[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XY['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XY['Class'].value_counts().plot(kind='pie', figsize=(7,7))\n",
    "_ = plt.title('Distribución de transacciones', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos, las clases están muy desbalanceadas ya que apenas hay casos fraudulentos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Division en features X + target Y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = XY.drop('Class', axis=1)\n",
    "Y = XY['Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ajuste del modelo de detección de anomalías a X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voy a utilizar el método Local Outlier Factor. Este método sólo se fija en los vecinos locales de cada punto por lo que vamos a ajustar el modelo a todos los datos X. \n",
    "\n",
    "Un parámetro que se puede ir cambiando es el **número de vecinos**. A menor número más se ajusta el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=LocalOutlierFactor(n_neighbors=10, \n",
    "                        algorithm='auto', \n",
    "                        leaf_size=30,\n",
    "                        metric='minkowski', \n",
    "                        p=2, \n",
    "                        metric_params=None, \n",
    "                        n_jobs=-1,\n",
    "                        novelty=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clf.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detección de anomalías"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo nos proporciona lo que se denominan factores de anomalías negativos. Cuanto más alto es este valor, más normal es el punto. Es decir, si queremos quedarnos con el 2 % de puntos más anómalos, debemos quedarnos con el 2 % de valores más bajo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factores_lof = clf.negative_outlier_factor_\n",
    "factores_lof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ponemos el umbral en un 2 % para tener margen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_clase = factores_lof.copy()\n",
    "Y_pred_clase[factores_lof>=np.percentile(factores_lof,2.)] = 0\n",
    "Y_pred_clase[factores_lof<np.percentile(factores_lof,2.)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " reporting_modelo(Y, Y_pred_clase) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos, con un dos por ciento de la muestra detectada como fraude, detectamos más del 50 % de las transacciones fraudulentas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representación de las probabilidades:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizo los factores al rango (0,1) para obtener una estimación de probabilidades:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_probs = NormalizeData(factores_lof)\n",
    "Y_pred_prob_pos = NormalizeData(factores_lof)[np.where(Y == 1)]\n",
    "Y_pred_prob_neg = NormalizeData(factores_lof)[np.where(Y == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repres_doble_hist(Y_pred_prob_pos, Y_pred_prob_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo asigna a la clase fraudulenta valores centrados en el 1 (anómalos). Por otro lado, la clase no fraudulenta tiene muchos valores en 0s y en 1s. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
